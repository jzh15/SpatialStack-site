<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="SpatialStack teaser page">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SpatialStack</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SpatialStack: Layered Geometry-Language Fusion for 3D VLM Spatial Reasoning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Authors</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <button class="button is-normal is-rounded" disabled>
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper (under review)</span>
                </button>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser" id="teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <img src="./static/images/fig1_teaser_v6.png" alt="SpatialStack teaser figure">
        <figcaption class="has-text-centered is-italic mt-2">
          <strong>SpatialStack: Layered Geometry-Language Fusion.</strong> Conventional VLMs (a) fuse a single deep geometry feature with vision tokens, limiting both fine-grained perception and high-level spatial reasoning. SpatialStack (b) stacks multi-level geometry features and injects them hierarchically into decoder layers, yielding stronger 3D spatial understanding across benchmarks.
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<section class="section" id="abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large vision-language models (VLMs) still struggle with reliable 3D spatial reasoning, a core capability for embodied and physical AI systems. This limitation arises from their inability to capture fine-grained 3D geometry and spatial relationships. While recent efforts have introduced multi-view geometry transformers into VLMs, they typically fuse only the deep-layer features from vision and geometry encoders, discarding rich hierarchical signals and creating a fundamental bottleneck for spatial understanding.
          </p>
          <p>
            To overcome this, we propose SpatialStack, a general hierarchical fusion framework that progressively aligns vision, geometry, and language representations across the model hierarchy. Moving beyond conventional late-stage vision-geometry fusion, SpatialStack stacks and synchronizes multi-level geometric features with the language backbone, enabling the model to capture both local geometric precision and global contextual semantics. Building upon this framework, we develop VLM-SpatialStack, a model that achieves state-of-the-art performance on multiple 3D spatial reasoning benchmarks. Extensive experiments and ablations demonstrate that our multi-level fusion strategy consistently enhances 3D understanding and generalizes robustly across diverse spatial reasoning tasks, establishing SpatialStack as an effective and extensible design paradigm for vision-language-geometry integration in next-generation multimodal physical AI systems.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="architecture">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Architecture</h2>
    <figure class="image">
      <img src="./static/images/fig2_arch_v1.png" alt="Architecture diagram">
      <figcaption class="has-text-centered is-italic mt-2"><strong>Architecture of SpatialStack.</strong> A standard VLM backbone is coupled with a multi-view geometry encoder whose layer-wise features are processed by lightweight projectors and sequentially injected into the decoder, progressively integrating geometric cues to preserve fine structure and global spatial context.</figcaption>
    </figure>
    <div class="content has-text-justified mt-4">
      <p>
        SpatialStack keeps the vision encoder unchanged and augments the language decoder with a parallel VGGT geometry stream. Geometry tokens from several semantic depths are compressed by lightweight mergers and injected as residual adapters throughout the decoder, so each stage reasons over aligned visual, geometric, and textual cues while preserving both local structure and global scene context.
      </p>
    </div>
  </div>
</section>

<section class="section is-light" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">VSI-Bench Results</h2>
    <div class="columns is-variable is-6 is-centered">
      <div class="column is-10">
        <div class="box">
          <div class="table-container">
            <table class="table is-striped is-fullwidth is-size-7">
              <thead>
                <tr>
                  <th>Model</th>
                  <th>Rank</th>
                  <th>Avg</th>
                  <th>Obj Count</th>
                  <th>Abs Dist</th>
                  <th>Obj Size</th>
                  <th>Room Size</th>
                  <th>Rel Dist</th>
                  <th>Rel Dir</th>
                  <th>Route Plan</th>
                  <th>Appr Order</th>
                </tr>
              </thead>
              <tbody>
                <tr><td>LongVILA-8B</td><td>14</td><td>21.6</td><td>29.1</td><td>9.1</td><td>16.7</td><td>0.0</td><td>29.6</td><td>30.7</td><td>32.5</td><td>25.5</td></tr>
                <tr><td>Qwen2.5-VL-3B</td><td>13</td><td>28.7</td><td>33.1</td><td>19.4</td><td>17.4</td><td>24.8</td><td>37.3</td><td>44.3</td><td>31.4</td><td>22.0</td></tr>
                <tr><td>VILA-1.5-8B</td><td>12</td><td>28.9</td><td>17.4</td><td>21.8</td><td>50.3</td><td>18.8</td><td>32.1</td><td>34.8</td><td>31.0</td><td>24.8</td></tr>
                <tr><td>LongVA-7B</td><td>11</td><td>29.2</td><td>38.0</td><td>16.6</td><td>38.9</td><td>22.2</td><td>33.1</td><td>43.3</td><td>25.4</td><td>15.7</td></tr>
                <tr><td>Qwen2.5-VL-7B</td><td>10</td><td>29.2</td><td>25.2</td><td>10.9</td><td>35.8</td><td>29.2</td><td>38.7</td><td>37.5</td><td>29.4</td><td>26.7</td></tr>
                <tr><td>VILA-1.5-40B</td><td>9</td><td>31.2</td><td>22.4</td><td>24.8</td><td>48.7</td><td>22.7</td><td>40.5</td><td>25.7</td><td>31.5</td><td>32.9</td></tr>
                <tr><td>LLaVA-OneVision-7B</td><td>8</td><td>32.4</td><td>47.7</td><td>20.2</td><td>47.4</td><td>12.3</td><td>42.5</td><td>35.2</td><td>29.4</td><td>24.4</td></tr>
                <tr><td>LLaVA-Video-7B</td><td>7</td><td>35.6</td><td>48.5</td><td>14.0</td><td>47.8</td><td>24.2</td><td>43.5</td><td>42.4</td><td>34.0</td><td>30.6</td></tr>
                <tr><td>LLaVA-OneVision-72B</td><td>6</td><td>40.2</td><td>43.5</td><td>23.9</td><td>57.6</td><td>37.5</td><td>42.5</td><td>39.9</td><td>32.5</td><td>44.6</td></tr>
                <tr><td>LLaVA-Video-72B</td><td>5</td><td>40.9</td><td>48.9</td><td>22.8</td><td>57.4</td><td>35.3</td><td>42.4</td><td>36.7</td><td>35.0</td><td>48.6</td></tr>
                <tr><td>Spatial-MLLM-4B</td><td>4</td><td>47.0</td><td>65.3</td><td>34.8</td><td>63.1</td><td>45.1</td><td>41.3</td><td>46.2</td><td>33.5</td><td>46.3</td></tr>
                <tr><td>VG-LLM-4B</td><td>3</td><td>47.3</td><td>66.0</td><td>37.8</td><td>55.2</td><td>59.2</td><td>44.6</td><td>45.6</td><td>33.5</td><td>36.4</td></tr>
                <tr><td>Cambrian-S-3B</td><td>2</td><td>57.3</td><td><strong>70.7</strong></td><td>40.6</td><td><strong>68.0</strong></td><td>46.3</td><td><strong>64.8</strong></td><td>61.9</td><td>27.3</td><td>78.8</td></tr>
                <tr class="has-background-link-light"><td>SpatialStack-4B</td><td>1</td><td><strong>60.9</strong></td><td>69.2</td><td><strong>45.4</strong></td><td>63.0</td><td><strong>63.2</strong></td><td>57.9</td><td><strong>68.4</strong></td><td><strong>40.2</strong></td><td><strong>79.6</strong></td></tr>
              </tbody>
            </table>
          </div>
          <p class="has-text-centered is-italic mt-2">
            <strong>VSI-Bench leaderboard.</strong> SpatialStack-4B reaches the top open-source average (60.9) and leads most sub-tasks.
          </p>
        </div>
      </div>
    </div>
    <p class="has-text-justified mt-4">
      VSI-Bench spans more than 5,000 egocentric QA pairs across eight categories: four numerical tasks (object count, absolute distance, object size, room size) and four multiple-choice tasks (relative distance, relative direction, route planning, appearance order). Despite having no route-planning supervision, SpatialStack generalizes strongly to that category while remaining competitive on the rest of the benchmark.
    </p>
  </div>
</section>

<section class="section" id="cvbench">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">CV-Bench Results</h2>
    <div class="box">
      <div class="table-container">
        <table class="table is-fullwidth is-size-7">
          <thead>
            <tr>
              <th>Model</th>
              <th>2D (%)</th>
              <th>3D (%)</th>
              <th>Avg. (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr><td colspan="4"><em>Proprietary Models (API)</em></td></tr>
            <tr><td>GPT-4o</td><td>74.8</td><td>83.0</td><td>78.9</td></tr>
            <tr><td colspan="4"><em>Open-source Models</em></td></tr>
            <tr><td>Mini-Gemini-HD-34B</td><td>71.5</td><td>79.2</td><td>75.4</td></tr>
            <tr><td>LLaVA-NeXT-34B</td><td>73.0</td><td>74.8</td><td>73.9</td></tr>
            <tr><td>Cambrian-1-34B</td><td>74.0</td><td>79.7</td><td>76.9</td></tr>
            <tr><td>SAT-LLaVA-Video-7B</td><td>73.0</td><td>83.8</td><td>78.4</td></tr>
            <tr><td>SPAR-8B</td><td>72.3</td><td>89.1</td><td>80.7</td></tr>
            <tr><td>Qwen2.5-VL-3B</td><td>67.9</td><td>70.4</td><td>69.2</td></tr>
            <tr><td>Qwen2.5-VL-7B</td><td>73.9</td><td>80.9</td><td>77.4</td></tr>
            <tr><td>Cambrian-S-7B</td><td>74.3</td><td>83.0</td><td>78.7</td></tr>
            <tr><td>Cambrian-S-3B</td><td><strong>76.1</strong></td><td>76.3</td><td>76.2</td></tr>
            <tr><td colspan="4"><em>Dual-Encoder MLLMs</em></td></tr>
            <tr><td>VG-LLM-4B</td><td>71.3</td><td>87.7</td><td>79.5</td></tr>
            <tr><td>VG-LLM-8B</td><td>72.2</td><td><strong>91.1</strong></td><td>81.7</td></tr>
            <tr><td>SpatialStack-4B</td><td>75.4</td><td>87.0</td><td>81.2</td></tr>
            <tr><td>SpatialStack-8B</td><td><strong>76.1</strong></td><td>90.8</td><td><strong>83.5</strong></td></tr>
          </tbody>
        </table>
      </div>
      <p class="has-text-centered is-italic mt-2"><strong>CV-Bench leaderboard.</strong> SpatialStack-4B and -8B outperform existing dual-encoder baselines across splits.</p>
    </div>
    <p class="has-text-justified mt-4">
      CV-Bench reformulates classical 2D relation/counting tasks and 3D depth/distance questions into QA evaluations. SpatialStack-4B already overtakes VG-LLM-4B on both splits, while SpatialStack-8B delivers the best overall accuracy (83.5) and matches the strongest reported 2D score.
    </p>
  </div>
</section>



</body>
</html>
